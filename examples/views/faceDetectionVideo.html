<!DOCTYPE html>
<html>
<head>
  <script src="face-api.js"></script>
  <script src="commons.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jsmpeg/0.1/jsmpg.js"></script>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/css/materialize.css">
  <script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/0.100.2/js/materialize.min.js"></script>
  <style type="text/css">
    .selectedFaces {
      left: 900px;
      top: 0px;
      margin-top: 50px;
      position: absolute;
    }

    #selectedFaces canvas {
      margin-top: 10px;
      margin-left: 10px;
    }

  </style>
</head>
<body>
  <div id="navbar"></div>
  <div class="center-content page-container">
    <!-- 
      Before starting this: Use https://github.com/klonikar/node-rtsp-stream examples/livestream.js ws "some video file" 10000 288 240 file
      Then append ?video=ws://localhost:10000 to the URL and load.
     -->
    <div class="progress" id="loader">
      <div class="indeterminate"></div>
    </div>
    <div style="position: relative" class="margin">
      <canvas id="inputVideo" width="288" height="240"></canvas>
      <!-- video src="media/bbt.mp4" onplay="onPlay(this)" id="inputVideo" autoplay muted></video -->
      <canvas id="overlay" />
    </div>
    <div id="facesContainer"></div>
    <div class="row side-by-side">
      <div class="row">
        <label for="minConfidence">Min Confidence:</label>
        <input disabled value="0.7" id="minConfidence" type="text" class="bold">
      </div>
      <button
        class="waves-effect waves-light btn"
        onclick="onDecreaseThreshold()"
      >
        <i class="material-icons left">-</i>
      </button>
      <button
        class="waves-effect waves-light btn"
        onclick="onIncreaseThreshold()"
      >
        <i class="material-icons left">+</i>
      </button>
    </div>
    <div class="row side-by-side">
      <div class="row">
        <label for="time">Time:</label>
        <input disabled value="-" id="time" type="text" class="bold">
      </div>
      <div class="row">
        <label for="fps">Estimated Fps:</label>
        <input disabled value="-" id="fps" type="text" class="bold">
      </div>
    </div>
  </div>
  <div class="selectedFaces" id="selectedFaces"> 
    <h3> Selected Faces </h3>
  </div>


  <script>
    let minConfidence = 0.7
    let modelLoaded = false
    let locations

    function getQueryParams() {
      var qs = window.location.search || document.location.search;
      qs = qs.split("+").join(" ");

      var params = {}, tokens,
          re = /[?&]?([^=]+)=([^&]*)/g;

      while (tokens = re.exec(qs)) {
          params[decodeURIComponent(tokens[1])]
              = decodeURIComponent(tokens[2]);
      }

      return params;
    }

    function onIncreaseThreshold() {
      minConfidence = Math.min(faceapi.round(minConfidence + 0.1), 1.0)
      $('#minConfidence').val(minConfidence)
    }

    function onDecreaseThreshold() {
      minConfidence = Math.max(faceapi.round(minConfidence - 0.1), 0.1)
      $('#minConfidence').val(minConfidence)
    }

    function displayTimeStats(timeInMs) {
      $('#time').val(`${timeInMs} ms`)
      $('#fps').val(`${faceapi.round(1000 / timeInMs)}`)
    }

    async function onPlay(videoEl) {
      if(videoEl.paused || videoEl.ended || !modelLoaded)
        return false

      const { width, height } = faceapi.getMediaDimensions(videoEl)
      const canvas = $('#overlay').get(0)
      canvas.width = width
      canvas.height = height

      const ts = Date.now()
      const input = await faceapi.toNetInput(videoEl)
      locations = await faceapi.locateFaces(input, minConfidence)
      const faceImages = await faceapi.extractFaces(input.inputs[0], locations)
      displayTimeStats(Date.now() - ts)

      faceapi.drawDetection('overlay', locations.map(det => det.forSize(width, height)))

      // detect landmarks and get the aligned face image bounding boxes
      const alignedFaceBoxes = await Promise.all(faceImages.map(
        async (faceCanvas, i) => {
          const faceLandmarks = await faceapi.detectLandmarks(faceCanvas)
          return faceLandmarks.align(locations[i])
        }
      ))
      const alignedFaceImages = await faceapi.extractFaces(input.inputs[0], alignedFaceBoxes)

      // free memory for input tensors
      input.dispose()

      $('#facesContainer').empty()
      faceImages.forEach(async (faceCanvas, i) => {
        $('#facesContainer').append(faceCanvas);
        $('#facesContainer').append(alignedFaceImages[i]);
        var clickHandler = function(elem) {
            if(elem.parentNode.id == 'facesContainer') {
              $('#selectedFaces').append(elem);
              $('#selectedFaces').append(document.createElement('br'));
            }
            else {
              elem.nextSibling.remove();
              elem.remove();
            }
          };

        alignedFaceImages[i].onclick = function() {
          return clickHandler(alignedFaceImages[i]);
        };
        faceCanvas.onclick = function() {
          return clickHandler(faceCanvas);
        };

      })

      setTimeout(() => onPlay(videoEl), 1000)
    }

    async function run() {
      await faceapi.loadFaceDetectionModel('/')
      await faceapi.loadFaceLandmarkModel('/')
      modelLoaded = true
      onPlay($('#inputVideo').get(0))
      $('#loader').hide()
    }

    $(document).ready(function() {
      var params = getQueryParams();
      var canvas = document.getElementById('inputVideo');
      var client = new WebSocket(params["video"]);
      var player = new jsmpeg(client, {
          canvas: canvas,
          videoBufferSize: '288*240'
      });
      renderNavBar('#navbar', 'face_detection_video');
      run();
    })
  </script>
</body>
</html>